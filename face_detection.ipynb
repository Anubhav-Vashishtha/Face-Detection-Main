{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in c:\\python311\\lib\\site-packages (8.2.84)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in c:\\python311\\lib\\site-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in c:\\python311\\lib\\site-packages (from ultralytics) (3.7.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in c:\\python311\\lib\\site-packages (from ultralytics) (10.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in c:\\python311\\lib\\site-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in c:\\python311\\lib\\site-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in c:\\python311\\lib\\site-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (2.3.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (0.18.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in c:\\python311\\lib\\site-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in c:\\python311\\lib\\site-packages (from ultralytics) (5.9.8)\n",
            "Requirement already satisfied: py-cpuinfo in c:\\python311\\lib\\site-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in c:\\python311\\lib\\site-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in c:\\python311\\lib\\site-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\python311\\lib\\site-packages (from ultralytics) (2.0.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\python311\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\python311\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.8.0->ultralytics) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.13.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\anubhav vashishtha\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = YOLO('./pretrained_model_on_custom_dataset/my_pretrained_model_100_epochs.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ohyMSFSI2kW",
        "outputId": "3e8ac9de-ef1c-465a-cc42-6881675ac9a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Anubhav Vashishtha\\\\Desktop\\\\Face-Detection-main\\\\im1.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mim1.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m      4\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mboxes\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:567\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:226\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:198\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    197\u001b[0m )\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mstream\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mscreenshot\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[0;32m    210\u001b[0m ):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\data\\build.py:187\u001b[0m, in \u001b[0;36mload_inference_source\u001b[1;34m(source, batch, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_inference_source\u001b[39m(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, vid_stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    Loads an inference source for object detection and applies necessary transformations.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m        dataset (Dataset): A dataset object for the specified input source.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     source, stream, screenshot, from_img, in_memory, tensor \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     source_type \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m SourceTypes(stream, screenshot, from_img, tensor)\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# Dataloader\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\data\\build.py:162\u001b[0m, in \u001b[0;36mcheck_source\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m    160\u001b[0m     in_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 162\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[43mautocast_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# convert all list elements to PIL or np arrays\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     from_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (Image\u001b[38;5;241m.\u001b[39mImage, np\u001b[38;5;241m.\u001b[39mndarray)):\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\data\\loaders.py:514\u001b[0m, in \u001b[0;36mautocast_list\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m source:\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, (\u001b[38;5;28mstr\u001b[39m, Path)):  \u001b[38;5;66;03m# filename or uri\u001b[39;00m\n\u001b[1;32m--> 514\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, (Image\u001b[38;5;241m.\u001b[39mImage, np\u001b[38;5;241m.\u001b[39mndarray)):  \u001b[38;5;66;03m# PIL or np Image\u001b[39;00m\n\u001b[0;32m    516\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(im)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\PIL\\Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Anubhav Vashishtha\\\\Desktop\\\\Face-Detection-main\\\\im1.jpg'"
          ]
        }
      ],
      "source": [
        "\n",
        "results = model([\"img1.jpeg\"])\n",
        "\n",
        "for result in results:\n",
        "    boxes = result.boxes\n",
        "    masks = result.masks\n",
        "    keypoints = result.keypoints\n",
        "    probs = result.probs\n",
        "    obb = result.obb\n",
        "    result.show()\n",
        "    result.save(filename=\"result2.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vin4FQSdlrk-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 face, 79.0ms\n",
            "Speed: 2.0ms preprocess, 79.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 94.2ms\n",
            "Speed: 1.0ms preprocess, 94.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 80.5ms\n",
            "Speed: 2.0ms preprocess, 80.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 91.0ms\n",
            "Speed: 1.0ms preprocess, 91.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 76.7ms\n",
            "Speed: 1.0ms preprocess, 76.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 78.7ms\n",
            "Speed: 1.0ms preprocess, 78.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 87.5ms\n",
            "Speed: 2.0ms preprocess, 87.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 77.2ms\n",
            "Speed: 1.0ms preprocess, 77.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 84.1ms\n",
            "Speed: 2.0ms preprocess, 84.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 79.4ms\n",
            "Speed: 2.0ms preprocess, 79.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 90.8ms\n",
            "Speed: 1.0ms preprocess, 90.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 76.5ms\n",
            "Speed: 1.0ms preprocess, 76.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 90.5ms\n",
            "Speed: 1.0ms preprocess, 90.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 93.8ms\n",
            "Speed: 2.0ms preprocess, 93.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 84.3ms\n",
            "Speed: 1.0ms preprocess, 84.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 75.6ms\n",
            "Speed: 1.0ms preprocess, 75.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 75.5ms\n",
            "Speed: 1.0ms preprocess, 75.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 82.0ms\n",
            "Speed: 1.0ms preprocess, 82.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 90.7ms\n",
            "Speed: 2.0ms preprocess, 90.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 80.8ms\n",
            "Speed: 1.0ms preprocess, 80.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 80.4ms\n",
            "Speed: 2.0ms preprocess, 80.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 77.5ms\n",
            "Speed: 1.0ms preprocess, 77.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 80.6ms\n",
            "Speed: 1.0ms preprocess, 80.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 73.2ms\n",
            "Speed: 1.0ms preprocess, 73.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 76.6ms\n",
            "Speed: 2.0ms preprocess, 76.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 76.5ms\n",
            "Speed: 1.0ms preprocess, 76.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 76.7ms\n",
            "Speed: 1.0ms preprocess, 76.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 81.8ms\n",
            "Speed: 1.0ms preprocess, 81.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 77.4ms\n",
            "Speed: 1.0ms preprocess, 77.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 94.1ms\n",
            "Speed: 1.0ms preprocess, 94.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 88.8ms\n",
            "Speed: 1.0ms preprocess, 88.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 83.0ms\n",
            "Speed: 1.0ms preprocess, 83.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 82.0ms\n",
            "Speed: 1.0ms preprocess, 82.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 79.8ms\n",
            "Speed: 1.0ms preprocess, 79.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 face, 77.8ms\n",
            "Speed: 1.0ms preprocess, 77.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 faces, 73.6ms\n",
            "Speed: 2.0ms preprocess, 73.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 faces, 74.5ms\n",
            "Speed: 2.0ms preprocess, 74.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        }
      ],
      "source": [
        "video_cap = cv2.VideoCapture(0)\n",
        "\n",
        "if not video_cap.isOpened():\n",
        "    print(\"Error: Could not open video source.\")\n",
        "else:\n",
        "    while True:\n",
        "        ret, frame = video_cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            print(\"Error: Failed to capture video frame.\")\n",
        "            break \n",
        "\n",
        "        results = model(frame)\n",
        "\n",
        "        for result in results[0].boxes:\n",
        "            x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "            conf = result.conf[0]  \n",
        "            cls = int(result.cls[0])  \n",
        "            label = f\"{model.names[cls]} {conf:.2f}\"\n",
        "            \n",
        "            \n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        \n",
        "        cv2.imshow('Video',frame)\n",
        "\n",
        "        if cv2.waitKey(10) == ord(\"a\"):\n",
        "            break\n",
        "\n",
        "    \n",
        "    video_cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
